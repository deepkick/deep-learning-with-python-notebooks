{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1408d6395a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 通常はデータをシャッフルするのが適している\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 検証データセットを定義\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# リスト4-1. ホールドアウト法\n",
    "num_validation_samples = 10000\n",
    "\n",
    "# 通常はデータをシャッフルするのが適している\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# 検証データセットを定義\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_datasamples:]\n",
    "\n",
    "# 訓練データセットを定義\n",
    "training_data = data[:]\n",
    "\n",
    "# モデルを訓練データで訓練し、検証データで評価\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "# この時点で、モデルのチューニング、再訓練、評価、再訓練...の繰り返しが可能となる\n",
    "\n",
    "# ハイパーパラメータのチューニングが済んだら、\n",
    "# テストにまったく使用していないデータで最終的なモデルの訓練を行うのが一般的　\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "test_score = model.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aec435f67fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# リスト4-2. k分割交差検証\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# リスト4-2. k分割交差検証\n",
    "k = 4\n",
    "num_validation_samples = len(data)\n",
    "np.random.shuffle(data)\n",
    "validation_scores = []\n",
    "\n",
    "for fold in range(k):\n",
    "    # 検証データを選択\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                           num_validation * (fold + 1)]\n",
    "    \n",
    "    # 残りのデータは訓練データとして使用\n",
    "    # +演算子は合計を求めるのではなくリストの連結を行うことに注意\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    \n",
    "    # モデルのまったく新しいインスタンスを作成\n",
    "    model = get_model()\n",
    "    # モデルを訓練\n",
    "    model.train(training_data)\n",
    "    \n",
    "    # 検証スコアはk個のフォールドの検証スコアの平均\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "    \n",
    "validation_score = np.average(validation_scores)\n",
    "# テストにまったく使用していないデータで最終的なモデルを訓練\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 データ前処理、特徴エンジニアリング、表現学習  \n",
    "\n",
    "ニューラルネットワークに供給する入力データと目的地をどのように準備すればよいかという問題。  \n",
    "ここでは、すべての問題領域のデータに共通する基本的な手法を紹介する。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 ニューラルネットワークでのデータ前処理  \n",
    "\n",
    "データ前処理の目的は、生のデータをニューラルネットワークにより適したものにすることです。これには、ベクトル化、正規化、欠損値の処理、特徴抽出が含まれる。\n",
    "\n",
    "- ベクトル化  \n",
    "ニューラルネットワークの入力値と目的地はすべて、浮動小数点データのテンソルでなければならない。  \n",
    "この手順を「データのベクトル化」(data vectorization)と呼びます。  \n",
    "例えば、整数のリストとして表されたテキストを、one-hotエンコーディングを使ってfloat型のテンソルに変換しました。  \n",
    "手書きの数字の分類と住宅価格の予測では、データはすでにベクトル化されていたため、この手順を省略した。  \n",
    "\n",
    "\n",
    "- 値の正規化  \n",
    "各特徴量を個別に正規化し、標準偏差が１、平均が0になるようにする必要がある。  \n",
    "NumPy配列を利用する。\n",
    "\n",
    "```\n",
    "# xは形状が()の2次元配列\n",
    "x -= x.mean(axis=0\n",
    "x /= x.std(axis=0)\n",
    "```\n",
    "\n",
    "\n",
    "- 欠損値の処理  \n",
    "一般にニューラルネットワークでは、欠損値は0にするのが安全である。  \n",
    "\n",
    "***\n",
    "#### 4.3.2 特徴エンジニアリング  \n",
    "\n",
    "特徴エンジニアリング（feature engineering）は、使用しているデータと機械学習アルゴリズムに関する知識に基づいて、そのアルゴリズムの性能を向上させるプロセスである。  \n",
    "従来のShallow Learningアルゴリズムの仮説空間は、それらのアルゴリズムが有益な特徴量を学習するのに十分ではなかった。このため、Deep Learningが登場する以前は、特徴エンジニアリングは非常に重要であった。  \n",
    "ニューラルネットワークは生のデータから有益な特徴量を自動的に抽出できるため、最近のDeep Learningでは、ほとんどの特徴エンジニアリングが必要ではなくなっている。しかし以下の利点を考慮すべきである。\n",
    "\n",
    "- よい特徴量があると、リソースの消費を抑えた上で、問題をより的確に解決できる。  \n",
    "- よい特徴量があると、問題をずっと少ないデータで解決できる。Deep Learningのモデルが特徴量から学習できるかどうかは、大量の訓練データが提供されるかどうかにかかっている。サンプルが少ししかない場合、特徴量の情報的価値はきわめて高くなる。  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
